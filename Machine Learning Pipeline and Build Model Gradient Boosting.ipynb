{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import spark library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .appName(\"Classification\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.load(\"D:/KULIAHHH/semester 8/BDATA/PROYEK/bank-additional-full.csv\",\n",
    "                     format=\"csv\", inferSchema=\"true\", header=\"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rename some variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed(\"emp.var.rate\",\"emp_var_rate\")\n",
    "df = df.withColumnRenamed(\"cons.price.idx\",\"cons_price_idx\")\n",
    "df = df.withColumnRenamed(\"cons.conf.idx\",\"cons_conf_idx\")\n",
    "df = df.withColumnRenamed(\"nr.employed\",\"nr_employed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select(\n",
    "    'age', 'job', 'marital', 'education', 'default', 'housing', 'loan', \n",
    "    'contact', 'duration', 'campaign', 'pdays', 'poutcome', 'emp_var_rate',\n",
    "    'cons_price_idx', 'cons_conf_idx', 'euribor3m', 'nr_employed', 'y'\n",
    ")\n",
    "cols = df.columns\n",
    "# df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode Categorical Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 tahap yang akan dilakukan adalah :\n",
    "- String Indexing, Konsepnya mirip dengan Label Encoding dimana setiap nilai unik pada fitur kategorikal akan diberi label dengan nilai numerik. \n",
    "\n",
    "- One-Hot Encoding, fitur yang sudah di indexing ditransformasi menggunakan OneHotEncoder sehingga output yang akan dihasilkan memberikan sebuah vektor biner. \n",
    "\n",
    "- Vector Assembler, menggabungkan semua fitur ke dalam sebuah kolom vektor yang sudah siap digunakan untuk membangun model machine learning. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "\n",
    "stages = []\n",
    "categoricalColumns = [\n",
    "    'job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'poutcome'\n",
    "]\n",
    "\n",
    "for categoricalCol in categoricalColumns:\n",
    "    # String Indexing\n",
    "    stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + 'Index')\n",
    "    # One Hot Encoding\n",
    "    encoder = OneHotEncoder(\n",
    "        inputCols=[stringIndexer.getOutputCol()], \n",
    "        outputCols=[categoricalCol + \"classVec\"]\n",
    "    )\n",
    "    stages += [stringIndexer, encoder]\n",
    "    \n",
    "label_stringIdx = StringIndexer(inputCol = 'y', outputCol = 'label')\n",
    "stages += [label_stringIdx]\n",
    "numericCols = ['age', 'duration', 'campaign', 'pdays', 'emp_var_rate', 'cons_price_idx', \n",
    "               'cons_conf_idx', 'euribor3m', 'nr_employed']\n",
    "\n",
    "# Vector Assembler\n",
    "assemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline and Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "# make a model\n",
    "stage_gbtClassifier = GBTClassifier(featuresCol='features',labelCol='label', maxIter=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- menambahkan model kedalam stages yang berisi tahapan string indexing, one hot encoding, dan label encoding.\n",
    "- memanggil pipeline model dan melakukan train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|(39,[8,11,18,21,2...|  0.0|[1.30176325134733...|[0.93108819481038...|       0.0|\n",
      "|(39,[3,11,15,22,2...|  0.0|[1.31292810233643...|[0.93250721620814...|       0.0|\n",
      "|(39,[3,11,15,21,2...|  0.0|[1.30573432492754...|[0.93159604511677...|       0.0|\n",
      "|(39,[0,11,19,21,2...|  0.0|[1.31292810233643...|[0.93250721620814...|       0.0|\n",
      "|(39,[3,11,15,21,2...|  0.0|[1.30047513377923...|[0.93092271222613...|       0.0|\n",
      "|(39,[3,11,16,22,2...|  0.0|[1.31292810233643...|[0.93250721620814...|       0.0|\n",
      "|(39,[0,11,17,21,2...|  0.0|[1.31292810233643...|[0.93250721620814...|       0.0|\n",
      "|(39,[1,11,20,22,2...|  0.0|[1.30573432492754...|[0.93159604511677...|       0.0|\n",
      "|(39,[2,12,17,21,2...|  0.0|[1.26977656877971...|[0.92686854256749...|       0.0|\n",
      "|(39,[3,12,15,21,2...|  0.0|[1.31292810233643...|[0.93250721620814...|       0.0|\n",
      "|(39,[1,11,20,22,2...|  0.0|[1.31292810233643...|[0.93250721620814...|       0.0|\n",
      "|(39,[3,12,15,21,2...|  0.0|[1.30573432492754...|[0.93159604511677...|       0.0|\n",
      "|(39,[1,12,15,21,2...|  0.0|[1.31292810233643...|[0.93250721620814...|       0.0|\n",
      "|(39,[8,13,18,21,2...|  0.0|[1.30047513377923...|[0.93092271222613...|       0.0|\n",
      "|(39,[1,11,19,21,2...|  0.0|[1.31292810233643...|[0.93250721620814...|       0.0|\n",
      "|(39,[5,11,16,22,2...|  0.0|[1.31292810233643...|[0.93250721620814...|       0.0|\n",
      "|(39,[1,11,19,21,2...|  0.0|[1.30047513377923...|[0.93092271222613...|       0.0|\n",
      "|(39,[1,11,19,22,2...|  0.0|[1.10268676700100...|[0.90073101923698...|       0.0|\n",
      "|(39,[1,11,16,21,2...|  0.0|[1.26977656877971...|[0.92686854256749...|       0.0|\n",
      "|(39,[4,12,16,22,2...|  0.0|[1.31292810233643...|[0.93250721620814...|       0.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stages.append(stage_gbtClassifier)\n",
    "pipeline = Pipeline(stages= stages)\n",
    "# gbt_pipeline = Pipeline(stages= [stages, stage_5])\n",
    "# fit the pipeline for the trainind data\n",
    "model = pipeline.fit(df)\n",
    "\n",
    "# transform the data\n",
    "sample_data_train = model.transform(df)\n",
    "\n",
    "# view some of the columns generated\n",
    "sample_data_train.select('features', 'label', 'rawPrediction', 'probability', 'prediction').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import dan preproses data test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = spark.read.load(\"D:/KULIAHHH/semester 8/BDATA/PROYEK/bank-additional-full.csv\",\n",
    "                     format=\"csv\", inferSchema=\"true\", header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.withColumnRenamed(\"emp.var.rate\",\"emp_var_rate\")\n",
    "test_df = test_df.withColumnRenamed(\"cons.price.idx\",\"cons_price_idx\")\n",
    "test_df = test_df.withColumnRenamed(\"cons.conf.idx\",\"cons_conf_idx\")\n",
    "test_df = test_df.withColumnRenamed(\"nr.employed\",\"nr_employed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.select(\n",
    "    'age', 'job', 'marital', 'education', 'default', 'housing', 'loan', \n",
    "    'contact', 'duration', 'campaign', 'pdays', 'poutcome', 'emp_var_rate',\n",
    "    'cons_price_idx', 'cons_conf_idx', 'euribor3m', 'nr_employed', 'y'\n",
    ")\n",
    "cols = test_df.columns\n",
    "# df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_test = model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(39,[8,11,18,21,2...|[1.30176325134733...|[0.93108819481038...|       0.0|\n",
      "|  0.0|(39,[3,11,15,22,2...|[1.31292810233643...|[0.93250721620814...|       0.0|\n",
      "|  0.0|(39,[3,11,15,21,2...|[1.30573432492754...|[0.93159604511677...|       0.0|\n",
      "|  0.0|(39,[0,11,19,21,2...|[1.31292810233643...|[0.93250721620814...|       0.0|\n",
      "|  0.0|(39,[3,11,15,21,2...|[1.30047513377923...|[0.93092271222613...|       0.0|\n",
      "|  0.0|(39,[3,11,16,22,2...|[1.31292810233643...|[0.93250721620814...|       0.0|\n",
      "|  0.0|(39,[0,11,17,21,2...|[1.31292810233643...|[0.93250721620814...|       0.0|\n",
      "|  0.0|(39,[1,11,20,22,2...|[1.30573432492754...|[0.93159604511677...|       0.0|\n",
      "|  0.0|(39,[2,12,17,21,2...|[1.26977656877971...|[0.92686854256749...|       0.0|\n",
      "|  0.0|(39,[3,12,15,21,2...|[1.31292810233643...|[0.93250721620814...|       0.0|\n",
      "|  0.0|(39,[1,11,20,22,2...|[1.31292810233643...|[0.93250721620814...|       0.0|\n",
      "|  0.0|(39,[3,12,15,21,2...|[1.30573432492754...|[0.93159604511677...|       0.0|\n",
      "|  0.0|(39,[1,12,15,21,2...|[1.31292810233643...|[0.93250721620814...|       0.0|\n",
      "|  0.0|(39,[8,13,18,21,2...|[1.30047513377923...|[0.93092271222613...|       0.0|\n",
      "|  0.0|(39,[1,11,19,21,2...|[1.31292810233643...|[0.93250721620814...|       0.0|\n",
      "|  0.0|(39,[5,11,16,22,2...|[1.31292810233643...|[0.93250721620814...|       0.0|\n",
      "|  0.0|(39,[1,11,19,21,2...|[1.30047513377923...|[0.93092271222613...|       0.0|\n",
      "|  0.0|(39,[1,11,19,22,2...|[1.10268676700100...|[0.90073101923698...|       0.0|\n",
      "|  0.0|(39,[1,11,16,21,2...|[1.26977656877971...|[0.92686854256749...|       0.0|\n",
      "|  0.0|(39,[4,12,16,22,2...|[1.31292810233643...|[0.93250721620814...|       0.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_data_test.select('label', 'features', 'rawPrediction', 'probability', 'prediction').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC: 0.9476449693695207\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "gbtEval = BinaryClassificationEvaluator()\n",
    "gbtROC = gbtEval.evaluate(sample_data_test, {gbtEval.metricName: \"areaUnderROC\"})\n",
    "print(\"Test Area Under ROC: \" + str(gbtROC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "Pkl_Filename = \"model.pkl\"  \n",
    "\n",
    "with open(Pkl_Filename, 'wb') as file:  \n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Model back from file\n",
    "with open(Pkl_Filename, 'rb') as file:  \n",
    "    Pickled_LR_Model = pickle.load(file)\n",
    "\n",
    "Pickled_LR_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
